# -*- coding: utf-8 -*-
"""handwritten_number_recognition_mnistdataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wYHg5MAUaMH3nZz6hN2oGjfMB78lQilb
"""

# prog classifies MNIST handwritten digit images 0-9
!pip install tensorflow keras numpy mnist matplotlib

#import the pakage / dependencies
import numpy as np
import mnist #get data set 
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical

# load dataset
train_images = mnist.train_images()
train_labels = mnist.train_labels()

test_images = mnist.test_images()
test_labels = mnist.test_labels()

#normalise the images, normalise the pixel values from [0,255] to [-0.5, 0.5] to make our network easier to train

train_images = (train_images/255) - 0.5

test_images = (test_images/255) - 0.5

#Flatten the images, each 28x28 images into a 28^2= 784 dimensional vector to pass into the neural network

train_images = train_images.reshape((-1,784))
test_images = test_images.reshape((-1,784))

#print the shape

print(train_images.shape) # 60000rows and 784 cols
print(test_images.shape)  #10000rows and 784 cols

# build model
# 3 layers: 2 layers -> 64neurons and relu function
#            1 layer  -> 10 neurons and softmax function

model = Sequential()
model.add( Dense(64, activation='relu', input_dim=784))
model.add( Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))

#compile model
# loss function measure how well the model did on training and tries to improve on it using thr optimiser

model.compile(
    optimizer = 'adam',
      loss = 'categorical_crossentropy', #(this function crossentropy is used for greater than 2 classes)
      metrics = ['accuracy']
)

# train model
 
model.fit(
    train_images,
      to_categorical(train_labels), # ex. 2 it expects[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
      epochs = 10, # number of iteration over the entire dataset to train on
      batch_size=32 #number of samples per gradient update for training
)

#evaluate teh model
model.evaluate(
    test_images,
    to_categorical(test_labels)
)

#model.save_weights('model.h5')

# predict on the first 5 test images
predictions = model.predict(test_images[0:5])
 
# print model predictions
print(np.argmax(predictions, axis = 1))
print(test_labels[5:10])

for i in range(0,5):
  first_image = test_images[i]
  first_image = np.array(first_image, dtype='float')
  pixels = first_image.reshape((28,28))
  plt.imshow(pixels, )
  plt.show()